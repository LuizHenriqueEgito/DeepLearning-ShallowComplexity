{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20ecb52d-20b3-4e24-8ba2-12598a9a1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from labml.logger import inspect\n",
    "from labml_nn.transformers.mha import MultiHeadAttention\n",
    "\n",
    "\n",
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d: int, base: int = 10_000):\n",
    "        \"\"\"\n",
    "        * `d` is the number of features $d$\n",
    "        * `base` is the constant used for calculating $\\Theta$\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.base = base\n",
    "        self.d = d\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "\n",
    "    def _build_cache(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Cache $\\cos$ and $\\sin$ values\n",
    "        \"\"\"\n",
    "        # Return if cache is already built\n",
    "        if self.cos_cached is not None and x.shape[0] <= self.cos_cached.shape[0]:\n",
    "            return\n",
    "\n",
    "        # Get sequence length\n",
    "        seq_len = x.shape[0]\n",
    "\n",
    "        # $\\Theta = {\\theta_i = 10000^{-\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
    "        theta = 1. / (self.base ** (torch.arange(0, self.d, 2).float() / self.d)).to(x.device)\n",
    "\n",
    "        # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
    "        seq_idx = torch.arange(seq_len, device=x.device).float().to(x.device)\n",
    "\n",
    "        # Calculate the product of position index and $\\theta_i$\n",
    "        idx_theta = torch.einsum('n,d->nd', seq_idx, theta)\n",
    "\n",
    "        # Concatenate so that for row $m$ we have\n",
    "        # $[m \\theta_0, m \\theta_1, ..., m \\theta_{\\frac{d}{2}}, m \\theta_0, m \\theta_1, ..., m \\theta_{\\frac{d}{2}}]$\n",
    "        idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1)\n",
    "\n",
    "        # Cache them\n",
    "        self.cos_cached = idx_theta2.cos()[:, None, None, :]\n",
    "        self.sin_cached = idx_theta2.sin()[:, None, None, :]\n",
    "\n",
    "    def _neg_half(self, x: torch.Tensor):\n",
    "        # $\\frac{d}{2}$\n",
    "        d_2 = self.d // 2\n",
    "\n",
    "        # Calculate $[-x^{(\\frac{d}{2} + 1)}, -x^{(\\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., x^{(\\frac{d}{2})}]$\n",
    "        return torch.cat([-x[:, :, :, d_2:], x[:, :, :, :d_2]], dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` is the Tensor at the head of a key or a query with shape `[seq_len, batch_size, n_heads, d]`\n",
    "        \"\"\"\n",
    "        # Cache $\\cos$ and $\\sin$ values\n",
    "        self._build_cache(x)\n",
    "\n",
    "        # Split the features, we can choose to apply rotary embeddings only to a partial set of features.\n",
    "        x_rope, x_pass = x[..., :self.d], x[..., self.d:]\n",
    "\n",
    "        # Calculate\n",
    "        # $[-x^{(\\frac{d}{2} + 1)}, -x^{(\\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., x^{(\\frac{d}{2})}]$\n",
    "        neg_half_x = self._neg_half(x_rope)\n",
    "\n",
    "        # Calculate\n",
    "        #\n",
    "        # \\begin{align}\n",
    "        # \\begin{pmatrix}\n",
    "        # x^{(i)}_m \\cos m \\theta_i - x^{(i + \\frac{d}{2})}_m \\sin m \\theta_i \\\\\n",
    "        # x^{(i + \\frac{d}{2})}_m \\cos m\\theta_i + x^{(i)}_m \\sin m \\theta_i \\\\\n",
    "        # \\end{pmatrix} \\\\\n",
    "        # \\end{align}\n",
    "        #\n",
    "        # for $i \\in {1, 2, ..., \\frac{d}{2}}$\n",
    "        x_rope = (x_rope * self.cos_cached[:x.shape[0]]) + (neg_half_x * self.sin_cached[:x.shape[0]])\n",
    "\n",
    "        #\n",
    "        return torch.cat((x_rope, x_pass), dim=-1)\n",
    "\n",
    "\n",
    "class RotaryPEMultiHeadAttention(MultiHeadAttention):\n",
    "    \"\"\"\n",
    "    ## Multi-head attention with rotary positional embeddings\n",
    "\n",
    "    We override [multi-head attention from original transformer](../mha.html).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, heads: int, d_model: int, rope_percentage: float = 0.5, dropout_prob: float = 0.0):\n",
    "        super().__init__(heads, d_model, dropout_prob)\n",
    "\n",
    "        # Rotary positional embedding layers\n",
    "        d_rope = int(self.d_k * rope_percentage)\n",
    "        self.query_rotary_pe = RotaryPositionalEmbeddings(d_rope)\n",
    "        self.key_rotary_pe = RotaryPositionalEmbeddings(d_rope)\n",
    "\n",
    "    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n",
    "        \"\"\"\n",
    "        ### Calculate scores between queries and keys\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate dot-product with RoPE\n",
    "        return torch.einsum('ibhd,jbhd->ijbh', self.query_rotary_pe(query), self.key_rotary_pe(key))\n",
    "\n",
    "\n",
    "def _test_rotary():\n",
    "    \"\"\"\n",
    "    Testing RoPE with a simple example\n",
    "    \"\"\"\n",
    "    x = torch.tensor([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]], dtype=torch.float)\n",
    "    x = x[:, None, None, :]\n",
    "    inspect(x)\n",
    "\n",
    "    rotary_pe = RotaryPositionalEmbeddings(4)\n",
    "    inspect(rotary_pe(x))\n",
    "    x = rotary_pe(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25150cde-7095-4b8f-bbd1-e4104e2e6f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750829307c0948a8a211211514dee276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<pre  style=\"overflow-x: scroll;\"><strong></strong><span style=\"color: #C5C1B4\">[</span><strong></â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[  1.0000,   2.0000,   3.0000,   4.0000]]],\n",
       "\n",
       "\n",
       "        [[[ -2.8876,   4.9298,   6.6077,   7.0496]]],\n",
       "\n",
       "\n",
       "        [[[-11.0967,   7.7984,   2.6198,  10.1580]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_rotary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bff8839e-2790-40cf-98c2-9bfaa1434896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class RotaryPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=512):\n",
    "        super(RotaryPositionalEncoding, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.register_buffer('position', torch.arange(max_len).float())\n",
    "        self.register_buffer('div_term', torch.exp(torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        position = self.position[:seq_len].unsqueeze(1)\n",
    "        div_term = self.div_term.unsqueeze(0)\n",
    "        \n",
    "        sinusoid = torch.zeros(seq_len, self.dim)\n",
    "        sinusoid[:, 0::2] = torch.sin(position * div_term)\n",
    "        sinusoid[:, 1::2] = torch.cos(position * div_term)\n",
    "        sinusoid = sinusoid.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        return sinusoid\n",
    "\n",
    "# Exemplo de uso\n",
    "dim = 16\n",
    "max_len = 100\n",
    "rope = RotaryPositionalEncoding(dim, max_len)\n",
    "x = torch.randn(1, 50, dim)  # batch_size, seq_len, dim\n",
    "pos_encodings = rope(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "551435db-5a76-4659-8eab-8ed909e8f455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  3.1098e-01,  9.5042e-01,  9.9833e-02,\n",
       "           9.9500e-01,  3.1618e-02,  9.9950e-01,  9.9998e-03,  9.9995e-01,\n",
       "           3.1623e-03,  9.9999e-01,  1.0000e-03,  1.0000e+00,  3.1623e-04,\n",
       "           1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  5.9113e-01,  8.0658e-01,  1.9867e-01,\n",
       "           9.8007e-01,  6.3203e-02,  9.9800e-01,  1.9999e-02,  9.9980e-01,\n",
       "           6.3245e-03,  9.9998e-01,  2.0000e-03,  1.0000e+00,  6.3246e-04,\n",
       "           1.0000e+00],\n",
       "         [ 1.4112e-01, -9.8999e-01,  8.1265e-01,  5.8275e-01,  2.9552e-01,\n",
       "           9.5534e-01,  9.4726e-02,  9.9550e-01,  2.9995e-02,  9.9955e-01,\n",
       "           9.4867e-03,  9.9995e-01,  3.0000e-03,  1.0000e+00,  9.4868e-04,\n",
       "           1.0000e+00],\n",
       "         [-7.5680e-01, -6.5364e-01,  9.5358e-01,  3.0114e-01,  3.8942e-01,\n",
       "           9.2106e-01,  1.2615e-01,  9.9201e-01,  3.9989e-02,  9.9920e-01,\n",
       "           1.2649e-02,  9.9992e-01,  4.0000e-03,  9.9999e-01,  1.2649e-03,\n",
       "           1.0000e+00],\n",
       "         [-9.5892e-01,  2.8366e-01,  9.9995e-01, -1.0342e-02,  4.7943e-01,\n",
       "           8.7758e-01,  1.5746e-01,  9.8753e-01,  4.9979e-02,  9.9875e-01,\n",
       "           1.5811e-02,  9.9988e-01,  5.0000e-03,  9.9999e-01,  1.5811e-03,\n",
       "           1.0000e+00],\n",
       "         [-2.7942e-01,  9.6017e-01,  9.4715e-01, -3.2080e-01,  5.6464e-01,\n",
       "           8.2534e-01,  1.8860e-01,  9.8205e-01,  5.9964e-02,  9.9820e-01,\n",
       "           1.8973e-02,  9.9982e-01,  6.0000e-03,  9.9998e-01,  1.8974e-03,\n",
       "           1.0000e+00],\n",
       "         [ 6.5699e-01,  7.5390e-01,  8.0042e-01, -5.9944e-01,  6.4422e-01,\n",
       "           7.6484e-01,  2.1956e-01,  9.7560e-01,  6.9943e-02,  9.9755e-01,\n",
       "           2.2134e-02,  9.9976e-01,  6.9999e-03,  9.9998e-01,  2.2136e-03,\n",
       "           1.0000e+00],\n",
       "         [ 9.8936e-01, -1.4550e-01,  5.7432e-01, -8.1863e-01,  7.1736e-01,\n",
       "           6.9671e-01,  2.5029e-01,  9.6817e-01,  7.9915e-02,  9.9680e-01,\n",
       "           2.5296e-02,  9.9968e-01,  7.9999e-03,  9.9997e-01,  2.5298e-03,\n",
       "           1.0000e+00],\n",
       "         [ 4.1212e-01, -9.1113e-01,  2.9126e-01, -9.5664e-01,  7.8333e-01,\n",
       "           6.2161e-01,  2.8078e-01,  9.5977e-01,  8.9879e-02,  9.9595e-01,\n",
       "           2.8457e-02,  9.9960e-01,  8.9999e-03,  9.9996e-01,  2.8460e-03,\n",
       "           1.0000e+00],\n",
       "         [-5.4402e-01, -8.3907e-01, -2.0684e-02, -9.9979e-01,  8.4147e-01,\n",
       "           5.4030e-01,  3.1098e-01,  9.5042e-01,  9.9833e-02,  9.9500e-01,\n",
       "           3.1617e-02,  9.9950e-01,  9.9998e-03,  9.9995e-01,  3.1623e-03,\n",
       "           9.9999e-01],\n",
       "         [-9.9999e-01,  4.4257e-03, -3.3057e-01, -9.4378e-01,  8.9121e-01,\n",
       "           4.5360e-01,  3.4088e-01,  9.4011e-01,  1.0978e-01,  9.9396e-01,\n",
       "           3.4778e-02,  9.9940e-01,  1.1000e-02,  9.9994e-01,  3.4785e-03,\n",
       "           9.9999e-01],\n",
       "         [-5.3657e-01,  8.4385e-01, -6.0768e-01, -7.9418e-01,  9.3204e-01,\n",
       "           3.6236e-01,  3.7043e-01,  9.2886e-01,  1.1971e-01,  9.9281e-01,\n",
       "           3.7938e-02,  9.9928e-01,  1.2000e-02,  9.9993e-01,  3.7947e-03,\n",
       "           9.9999e-01],\n",
       "         [ 4.2017e-01,  9.0745e-01, -8.2453e-01, -5.6582e-01,  9.6356e-01,\n",
       "           2.6750e-01,  3.9961e-01,  9.1668e-01,  1.2963e-01,  9.9156e-01,\n",
       "           4.1098e-02,  9.9916e-01,  1.3000e-02,  9.9992e-01,  4.1110e-03,\n",
       "           9.9999e-01],\n",
       "         [ 9.9061e-01,  1.3674e-01, -9.5961e-01, -2.8135e-01,  9.8545e-01,\n",
       "           1.6997e-01,  4.2840e-01,  9.0359e-01,  1.3954e-01,  9.9022e-01,\n",
       "           4.4257e-02,  9.9902e-01,  1.4000e-02,  9.9990e-01,  4.4272e-03,\n",
       "           9.9999e-01],\n",
       "         [ 6.5029e-01, -7.5969e-01, -9.9952e-01,  3.1022e-02,  9.9749e-01,\n",
       "           7.0737e-02,  4.5675e-01,  8.8959e-01,  1.4944e-01,  9.8877e-01,\n",
       "           4.7416e-02,  9.9888e-01,  1.4999e-02,  9.9989e-01,  4.7434e-03,\n",
       "           9.9999e-01],\n",
       "         [-2.8790e-01, -9.5766e-01, -9.4031e-01,  3.4032e-01,  9.9957e-01,\n",
       "          -2.9199e-02,  4.8465e-01,  8.7471e-01,  1.5932e-01,  9.8723e-01,\n",
       "           5.0575e-02,  9.9872e-01,  1.5999e-02,  9.9987e-01,  5.0596e-03,\n",
       "           9.9999e-01],\n",
       "         [-9.6140e-01, -2.7516e-01, -7.8785e-01,  6.1586e-01,  9.9166e-01,\n",
       "          -1.2884e-01,  5.1206e-01,  8.5895e-01,  1.6918e-01,  9.8558e-01,\n",
       "           5.3733e-02,  9.9856e-01,  1.6999e-02,  9.9986e-01,  5.3758e-03,\n",
       "           9.9999e-01],\n",
       "         [-7.5099e-01,  6.6032e-01, -5.5726e-01,  8.3034e-01,  9.7385e-01,\n",
       "          -2.2720e-01,  5.3897e-01,  8.4233e-01,  1.7903e-01,  9.8384e-01,\n",
       "           5.6890e-02,  9.9838e-01,  1.7999e-02,  9.9984e-01,  5.6921e-03,\n",
       "           9.9998e-01],\n",
       "         [ 1.4988e-01,  9.8870e-01, -2.7141e-01,  9.6246e-01,  9.4630e-01,\n",
       "          -3.2329e-01,  5.6533e-01,  8.2487e-01,  1.8886e-01,  9.8200e-01,\n",
       "           6.0047e-02,  9.9820e-01,  1.8999e-02,  9.9982e-01,  6.0083e-03,\n",
       "           9.9998e-01],\n",
       "         [ 9.1295e-01,  4.0808e-01,  4.1358e-02,  9.9914e-01,  9.0930e-01,\n",
       "          -4.1615e-01,  5.9113e-01,  8.0658e-01,  1.9867e-01,  9.8007e-01,\n",
       "           6.3203e-02,  9.9800e-01,  1.9999e-02,  9.9980e-01,  6.3245e-03,\n",
       "           9.9998e-01],\n",
       "         [ 8.3666e-01, -5.4773e-01,  3.5002e-01,  9.3674e-01,  8.6321e-01,\n",
       "          -5.0485e-01,  6.1633e-01,  7.8749e-01,  2.0846e-01,  9.7803e-01,\n",
       "           6.6359e-02,  9.9780e-01,  2.0998e-02,  9.9978e-01,  6.6407e-03,\n",
       "           9.9998e-01],\n",
       "         [-8.8513e-03, -9.9996e-01,  6.2398e-01,  7.8144e-01,  8.0850e-01,\n",
       "          -5.8850e-01,  6.4092e-01,  7.6760e-01,  2.1823e-01,  9.7590e-01,\n",
       "           6.9514e-02,  9.9758e-01,  2.1998e-02,  9.9976e-01,  6.9570e-03,\n",
       "           9.9998e-01],\n",
       "         [-8.4622e-01, -5.3283e-01,  8.3606e-01,  5.4865e-01,  7.4571e-01,\n",
       "          -6.6628e-01,  6.6487e-01,  7.4696e-01,  2.2798e-01,  9.7367e-01,\n",
       "           7.2668e-02,  9.9736e-01,  2.2998e-02,  9.9974e-01,  7.2732e-03,\n",
       "           9.9997e-01],\n",
       "         [-9.0558e-01,  4.2418e-01,  9.6522e-01,  2.6144e-01,  6.7546e-01,\n",
       "          -7.3739e-01,  6.8816e-01,  7.2556e-01,  2.3770e-01,  9.7134e-01,\n",
       "           7.5822e-02,  9.9712e-01,  2.3998e-02,  9.9971e-01,  7.5894e-03,\n",
       "           9.9997e-01],\n",
       "         [-1.3235e-01,  9.9120e-01,  9.9866e-01, -5.1689e-02,  5.9847e-01,\n",
       "          -8.0114e-01,  7.1075e-01,  7.0344e-01,  2.4740e-01,  9.6891e-01,\n",
       "           7.8975e-02,  9.9688e-01,  2.4997e-02,  9.9969e-01,  7.9056e-03,\n",
       "           9.9997e-01],\n",
       "         [ 7.6256e-01,  6.4692e-01,  9.3307e-01, -3.5969e-01,  5.1550e-01,\n",
       "          -8.5689e-01,  7.3264e-01,  6.8062e-01,  2.5708e-01,  9.6639e-01,\n",
       "           8.2127e-02,  9.9662e-01,  2.5997e-02,  9.9966e-01,  8.2218e-03,\n",
       "           9.9997e-01],\n",
       "         [ 9.5638e-01, -2.9214e-01,  7.7495e-01, -6.3203e-01,  4.2738e-01,\n",
       "          -9.0407e-01,  7.5379e-01,  6.5711e-01,  2.6673e-01,  9.6377e-01,\n",
       "           8.5278e-02,  9.9636e-01,  2.6997e-02,  9.9964e-01,  8.5380e-03,\n",
       "           9.9996e-01],\n",
       "         [ 2.7091e-01, -9.6261e-01,  5.3997e-01, -8.4168e-01,  3.3499e-01,\n",
       "          -9.4222e-01,  7.7419e-01,  6.3295e-01,  2.7636e-01,  9.6106e-01,\n",
       "           8.8428e-02,  9.9608e-01,  2.7996e-02,  9.9961e-01,  8.8543e-03,\n",
       "           9.9996e-01],\n",
       "         [-6.6363e-01, -7.4806e-01,  2.5145e-01, -9.6787e-01,  2.3925e-01,\n",
       "          -9.7096e-01,  7.9382e-01,  6.0816e-01,  2.8595e-01,  9.5824e-01,\n",
       "           9.1578e-02,  9.9580e-01,  2.8996e-02,  9.9958e-01,  9.1705e-03,\n",
       "           9.9996e-01],\n",
       "         [-9.8803e-01,  1.5425e-01, -6.2015e-02, -9.9808e-01,  1.4112e-01,\n",
       "          -9.8999e-01,  8.1265e-01,  5.8275e-01,  2.9552e-01,  9.5534e-01,\n",
       "           9.4726e-02,  9.9550e-01,  2.9995e-02,  9.9955e-01,  9.4867e-03,\n",
       "           9.9995e-01],\n",
       "         [-4.0404e-01,  9.1474e-01, -3.6933e-01, -9.2930e-01,  4.1581e-02,\n",
       "          -9.9914e-01,  8.3067e-01,  5.5677e-01,  3.0506e-01,  9.5233e-01,\n",
       "           9.7874e-02,  9.9520e-01,  3.0995e-02,  9.9952e-01,  9.8029e-03,\n",
       "           9.9995e-01],\n",
       "         [ 5.5143e-01,  8.3422e-01, -6.4001e-01, -7.6837e-01, -5.8374e-02,\n",
       "          -9.9829e-01,  8.4786e-01,  5.3023e-01,  3.1457e-01,  9.4924e-01,\n",
       "           1.0102e-01,  9.9488e-01,  3.1995e-02,  9.9949e-01,  1.0119e-02,\n",
       "           9.9995e-01],\n",
       "         [ 9.9991e-01, -1.3277e-02, -8.4722e-01, -5.3124e-01, -1.5775e-01,\n",
       "          -9.8748e-01,  8.6420e-01,  5.0315e-01,  3.2404e-01,  9.4604e-01,\n",
       "           1.0417e-01,  9.9456e-01,  3.2994e-02,  9.9946e-01,  1.0435e-02,\n",
       "           9.9995e-01],\n",
       "         [ 5.2908e-01, -8.4857e-01, -9.7042e-01, -2.4142e-01, -2.5554e-01,\n",
       "          -9.6680e-01,  8.7967e-01,  4.7558e-01,  3.3349e-01,  9.4275e-01,\n",
       "           1.0731e-01,  9.9423e-01,  3.3993e-02,  9.9942e-01,  1.0752e-02,\n",
       "           9.9994e-01],\n",
       "         [-4.2818e-01, -9.0369e-01, -9.9738e-01,  7.2335e-02, -3.5078e-01,\n",
       "          -9.3646e-01,  8.9427e-01,  4.4753e-01,  3.4290e-01,  9.3937e-01,\n",
       "           1.1045e-01,  9.9388e-01,  3.4993e-02,  9.9939e-01,  1.1068e-02,\n",
       "           9.9994e-01],\n",
       "         [-9.9178e-01, -1.2796e-01, -9.2543e-01,  3.7892e-01, -4.4252e-01,\n",
       "          -8.9676e-01,  9.0797e-01,  4.1903e-01,  3.5227e-01,  9.3590e-01,\n",
       "           1.1360e-01,  9.9353e-01,  3.5992e-02,  9.9935e-01,  1.1384e-02,\n",
       "           9.9994e-01],\n",
       "         [-6.4354e-01,  7.6541e-01, -7.6171e-01,  6.4792e-01, -5.2984e-01,\n",
       "          -8.4810e-01,  9.2077e-01,  3.9011e-01,  3.6162e-01,  9.3233e-01,\n",
       "           1.1674e-01,  9.9316e-01,  3.6992e-02,  9.9932e-01,  1.1700e-02,\n",
       "           9.9993e-01],\n",
       "         [ 2.9637e-01,  9.5507e-01, -5.2244e-01,  8.5267e-01, -6.1186e-01,\n",
       "          -7.9097e-01,  9.3264e-01,  3.6081e-01,  3.7092e-01,  9.2866e-01,\n",
       "           1.1988e-01,  9.9279e-01,  3.7991e-02,  9.9928e-01,  1.2016e-02,\n",
       "           9.9993e-01],\n",
       "         [ 9.6380e-01,  2.6664e-01, -2.3137e-01,  9.7287e-01, -6.8777e-01,\n",
       "          -7.2593e-01,  9.4358e-01,  3.3114e-01,  3.8019e-01,  9.2491e-01,\n",
       "           1.2302e-01,  9.9240e-01,  3.8990e-02,  9.9924e-01,  1.2333e-02,\n",
       "           9.9992e-01],\n",
       "         [ 7.4511e-01, -6.6694e-01,  8.2646e-02,  9.9658e-01, -7.5680e-01,\n",
       "          -6.5364e-01,  9.5358e-01,  3.0114e-01,  3.8942e-01,  9.2106e-01,\n",
       "           1.2615e-01,  9.9201e-01,  3.9989e-02,  9.9920e-01,  1.2649e-02,\n",
       "           9.9992e-01],\n",
       "         [-1.5862e-01, -9.8734e-01,  3.8847e-01,  9.2146e-01, -8.1828e-01,\n",
       "          -5.7482e-01,  9.6263e-01,  2.7084e-01,  3.9861e-01,  9.1712e-01,\n",
       "           1.2929e-01,  9.9161e-01,  4.0989e-02,  9.9916e-01,  1.2965e-02,\n",
       "           9.9992e-01],\n",
       "         [-9.1652e-01, -3.9999e-01,  6.5576e-01,  7.5497e-01, -8.7158e-01,\n",
       "          -4.9026e-01,  9.7071e-01,  2.4027e-01,  4.0776e-01,  9.1309e-01,\n",
       "           1.3243e-01,  9.9119e-01,  4.1988e-02,  9.9912e-01,  1.3281e-02,\n",
       "           9.9991e-01],\n",
       "         [-8.3177e-01,  5.5511e-01,  8.5803e-01,  5.1360e-01, -9.1617e-01,\n",
       "          -4.0080e-01,  9.7782e-01,  2.0945e-01,  4.1687e-01,  9.0897e-01,\n",
       "           1.3556e-01,  9.9077e-01,  4.2987e-02,  9.9908e-01,  1.3597e-02,\n",
       "           9.9991e-01],\n",
       "         [ 1.7702e-02,  9.9984e-01,  9.7521e-01,  2.2130e-01, -9.5160e-01,\n",
       "          -3.0733e-01,  9.8395e-01,  1.7843e-01,  4.2594e-01,  9.0475e-01,\n",
       "           1.3869e-01,  9.9034e-01,  4.3986e-02,  9.9903e-01,  1.3914e-02,\n",
       "           9.9990e-01],\n",
       "         [ 8.5090e-01,  5.2532e-01,  9.9567e-01, -9.2948e-02, -9.7753e-01,\n",
       "          -2.1080e-01,  9.8910e-01,  1.4723e-01,  4.3497e-01,  9.0045e-01,\n",
       "           1.4182e-01,  9.8989e-01,  4.4985e-02,  9.9899e-01,  1.4230e-02,\n",
       "           9.9990e-01],\n",
       "         [ 9.0179e-01, -4.3218e-01,  9.1740e-01, -3.9798e-01, -9.9369e-01,\n",
       "          -1.1215e-01,  9.9326e-01,  1.1589e-01,  4.4395e-01,  8.9605e-01,\n",
       "           1.4495e-01,  9.8944e-01,  4.5984e-02,  9.9894e-01,  1.4546e-02,\n",
       "           9.9989e-01],\n",
       "         [ 1.2357e-01, -9.9234e-01,  7.4814e-01, -6.6354e-01, -9.9992e-01,\n",
       "          -1.2389e-02,  9.9643e-01,  8.4425e-02,  4.5289e-01,  8.9157e-01,\n",
       "           1.4808e-01,  9.8898e-01,  4.6983e-02,  9.9890e-01,  1.4862e-02,\n",
       "           9.9989e-01],\n",
       "         [-7.6825e-01, -6.4014e-01,  5.0470e-01, -8.6330e-01, -9.9616e-01,\n",
       "           8.7499e-02,  9.9860e-01,  5.2878e-02,  4.6178e-01,  8.8699e-01,\n",
       "           1.5121e-01,  9.8850e-01,  4.7982e-02,  9.9885e-01,  1.5178e-02,\n",
       "           9.9988e-01],\n",
       "         [-9.5375e-01,  3.0059e-01,  2.1120e-01, -9.7744e-01, -9.8245e-01,\n",
       "           1.8651e-01,  9.9977e-01,  2.1279e-02,  4.7063e-01,  8.8233e-01,\n",
       "           1.5433e-01,  9.8802e-01,  4.8980e-02,  9.9880e-01,  1.5495e-02,\n",
       "           9.9988e-01]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e7856d-7a91-453c-82a0-33e2fdaa08b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "env_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
